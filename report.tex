\documentclass{article}

\usepackage{enumerate}
\usepackage[bottom=1in,top=1in]{geometry}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage[pdftex,colorlinks,urlcolor=blue]{hyperref}

\geometry{letterpaper}

\begin{document}

\title{CS276 PA4 Report}

\author{
  Jiawei Yao\\
  \texttt{jwyao@stanford.edu}
  \and
  Wei Wei\\
  \texttt{wwei2@stanford.edu}
}

\maketitle

\section{Task 1 - Pointwise Approach with Linear Regression}

NDCG on train and dev datasets:

\begin{table}[!htb]
    \centering
    \begin{tabular}{| r | l | l |}
        \hline
        & \textbf{Train} & \textbf {Dev.} \\
        \hline
        \textbf{Linear Regression} & 0.8705 & \textbf{0.8420} \\
        \hline
    \end{tabular}
    \caption{NDCG on Task 1}
\end{table}

In pointwise approach, we use \texttt{LinearRegression} model with TF-IDF features.
We apply the following tweaks to TF-IDF features:

\begin{itemize}
    \item add-one smoothing on IDF
    \item sublinear on raw term frequencies
    \item body length normalization with added length of 900
\end{itemize}

After the model has been trained, for documents of given query, we use \texttt{Collections.sort} on the documents with a comparator such that
for two documents $d_1,d_2$, the one with higher score from the trained Linear Regression model is ranked higher.

\section{Task 2 - Pairwise Approach and Ranking SVM}

NDCG on train and dev datasets:

\begin{table}[!htb]
    \centering
    \begin{tabular}{| r | l | l |}
        \hline
        & \textbf{Train} & \textbf {Dev.} \\
        \hline
        \textbf{Linear SVM} & 0.8653 & \textbf{0.8463} \\
        \hline
        \textbf{RBF SVM} & 0.8660 & 0.8458 \\
        \hline
    \end{tabular}
    \caption{NDCG on Task 2}
\end{table}

For this task, we use the same TF-IDF features as in Task 1 with the same tweaks.

When training SVM, the following techniques are used:

\begin{itemize}
    \item use \texttt{Standardize} to standardize document features (not the differences)
    \item generate evenly distribtued features for each label by computing differences of document features
\end{itemize}

\texttt{Collections.sort} is used to rank documents of a same query.
For documents $d_1,d_2$, the comparator

\begin{enumerate}
    \item standardizes the document features with \emph{the same} \texttt{Standardize} from training
    \item computes the difference
    \item feeds the difference vector to the trained SVM
        \begin{itemize}
            \item if the label output is positive, $d_1$ is ranked higher
            \item otherwise $d_1$ is considered as \emph{not higher than} $d_2$\footnote{In this case, we let the sort algorithm to break tie.}
        \end{itemize}
\end{enumerate}

For Linear SVM, default parameters are used. For RBF SVM, we set $C=8.0,\gamma=0.001$.

\section{Task 3 - More Features and Error Analysis}

First, our NDCG performance train and dev datasets:

\begin{table}[!htb]
    \centering
    \begin{tabular}{| r | l | l |}
        \hline
        & \textbf{Train} & \textbf {Dev.} \\
        \hline
        \textbf{Local} & 0.8741 & \textbf{0.8624} \\
        \hline
        \textbf{CORN} & 0.8800 & 0.8598 \\
        \hline
    \end{tabular}
    \caption{NDCG on Task 3}
\end{table}

\textbf{Note}: We find that with the same parameter setting, the NDCG scores on our local machine (Mac OSX, Java 8) are different from NDCG scores on CORN machines. We don't know what the underlying reason is. Anyway, we also did grid search with different paramters on CORN machines, which finally gives us an NDCG score very close to 0.86. As a result, the Local row is the best result we achieved on our local machines (RBF kernel, $C=100,\gamma=0.001$) while the CORN row is the best result we achieved on CORN machines (RBF kernel, $C=34,\gamma=0.01005$).

\subsection{Implementation Decisions}

\begin{itemize}
    \item Use BM25 with pagerank. BM25 weights are reused from PA3. Using BM25 without PageRank give us lower score 0.8523 compared to 0.8556 using PageRank.
    \item Adjusting $K_1$ in BM25 doesn't give us significant increase in NDCG score. We tried 1, 1.5, 2, 2.5, and 3, and they all gave us similar scores. The highest is 0.8560 while the lowest is 0.8556.
    \item Use raw PageRank instead of log(PageRank). The NDCG results are similar so we use raw PageRank for simplicity.
\end{itemize}

\subsection{Combination of Suggested Features}

We experimented with three suggested features first. As we can see BM25 is the most important feature. Only using BM25 can boost performance by 0.01. Smallest window is not a very appealing feature. (In PA3 smallest window also provides marginal performance boost.)

NDCG scores with different feature combinations are shown in the following table\footnote{As Non-linear SVM requires grid search, we only report NDCG scores with linear SVMs.}:

\begin{table}[!htb]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{BM25} & \multirow{2}{*}{window} & \multirow{2}{*}{pagerank} & \multicolumn{2}{c|}{Linear SVM} & \multicolumn{2}{c|}{Non-linear SVM} \\ \cline{4-7}
                          & & & train & dev & train & dev \\ \cline{4-7}
    \hline
    \checkmark & & & 0.8716 & 0.8557 & - & - \\
    \hline
    & \checkmark & & 0.8653 & 0.8462 & - & - \\
    \hline
    & & \checkmark & 0.8736 & 0.8522 & - & - \\
    \hline
    \checkmark & \checkmark & & 0.8716 & 0.8553 & - & - \\
    \hline
    & \checkmark & \checkmark & 0.8737 & 0.8516 & - & - \\
    \hline
    \checkmark & & \checkmark & 0.8706 & 0.8556 & - & - \\
    \hline
    \checkmark & \checkmark & \checkmark & 0.8706 & 0.8556 & 0.8655 & 0.8443\\
    \hline
  \end{tabular}
  \caption{Results for task 3 - NDCG with different feature combinations}
\end{table}

\subsection{Systematic Errors Analysis}

After experimenting with three suggested features, we found three types of systematic errors.

\begin{enumerate}
  \item Our system favors long URLs that have higher term count.
    \begin{itemize}
      \item For query ``mscs program sheet", our system ranks \url{http://cs.stanford.edu/degrees/mscs/programsheets/09-10/MSCS-0910-RWC.pdf} the highest and \url{http://cs.stanford.edu/degrees/mscs/programsheets/} only ranks the 4th. The first URL is more specific and contains information about RWC track and it is for 09-10 academic years. The first link is too specific while the second one is just right. Hence, we take URL length into consideration by adding a new numeric feature.
    \end{itemize}
\item Our system gives ``.pdf" file pretty low score. Thus we add a binary feature to indicate whether a document name ends with ``.pdf". However, after more careful investination, we found that this assumption is not rock-solid because some raters doesn't like files (including PDFs), even though actually they are highly relevant documents.
  \item Our system doesn't take current time into consideration. For example, if user searches for 2014 academic calendar, we should give 2012-2013 academic calendar a high score. However, we don't add this one as a new feature because it's relatively hard to previsely extract.
\end{enumerate}

\subsection{More Features and Discussion}

We took three additional features into consideration: 1) number of fields where any query word appear, 2) URL length and 3) body length.

\textbf{Number of fields where any query word appear} is an extremely powerful feature and boosts the performance a lot. Intuitively, if all five fields contains some query words, the document may be more relevant.

\textbf{URL length} makes sense because shorter URL tends to be better based on a first observation. For example, given two URLs \url{http://nlp.stanford.edu/manning/tex/} and \url{http://nlp.stanford.edu/manning/tex/avm.sty} and query \texttt{christopher manning latex macros}, the former URL is shorter than the latter while the former contains all the key word of the latter. The latter may be more specific and contains less information of our information need.

Considering \textbf{body length} is reasonable because if one document has long body length but has the same body hits as a shorter document, the shorter document tends to be more relevant.

We did an ablation test on our proposed additional features using linear SVM. (We don't use RBF kernel because we need to do grid search for each combination which is very time consuming).

\begin{table}[!htb]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Num Fields} & \multirow{2}{*}{Body length} & \multirow{2}{*}{URL length} & \multicolumn{2}{c|}{Linear SVM} & \multicolumn{2}{c|}{Non-linear SVM} \\ \cline{4-7}
                          & & & train & dev & train & dev \\ \cline{4-7}
    \hline
    \checkmark & & & 0.8755 & 0.8562 & 0.8741 & \textbf{0.8624} \\
    \hline
    & \checkmark & & 0.8722 & 0.8554 & - & - \\
    \hline
    & & \checkmark & 0.8762 & 0.8540 & - & - \\
    \hline
    \checkmark & \checkmark & & 0.8767 & 0.8563 & - & - \\
    \hline
    & \checkmark & \checkmark & 0.8762 & 0.8547 & - & - \\
    \hline
    \checkmark & & \checkmark & 0.8744 & 0.8536 & - & - \\
    \hline
    \checkmark & \checkmark & \checkmark & 0.8753 & 0.8527 & 0.8763 & 0.8589\\
    \hline
  \end{tabular}
  \caption{Ablation Test Based On Linear SVM}
  \label{table:ablation}
\end{table}

As can be seen from Table.~(\ref{table:ablation}):
\begin{itemize}
   \item If we only add Num Fields the performance can be increased a lot. Indeed, after doing a grid search, we found a set of good parameters which gives a local maxima.
   \item Body length seems a promising feature because when we remove it we get poorer performance. However, it doesn't work well with non-linear SVM. An explanation might be shorter body length doesn't necessarily indicate higher relevance and longer body length doesn't imply more information. As an afterthought, we may try log(bodyLength) in the future.
   \item URL length is not a good feature. When we remove the URL from features, we get the highest NDCG among all others. And URL length alone gives us poor performance. The reason why URL length doesn't work well as expected is that users tend to issue specific queries but pages with short URLs tend to have generic information.
 \end{itemize}

\section{Extra Credit}

NDCG on train and dev datasets (numbers in parentheses are improvement over Task 1):

\begin{table}[!htb]
    \centering
    \begin{tabular}{| r | l | l |}
        \hline
        & \textbf{Train} & \textbf {Dev.} \\
        \hline
        \textbf{$\nu$-SVR} & 0.8669 ($-0.42\%$) & \textbf{0.8483} ($+0.75\%$) \\
        \hline
        \textbf{$\nu$-SVR w/ Std.} & 0.8770 ($+0.74\%$) & 0.8328 ($-1.09\%$) \\
        \hline
    \end{tabular}
    \caption{NDCG on Extra Credit}
\end{table}

For extra credit, we use SVM regression feature provided for the pointwise approach.
Specifically, we use $\nu$-SVR SVM with RBF kernel, $C=16.0,\nu=0.25$. We also tried standardizing data but that was not as good as expected (see table above).

We can see that $\nu$-SVR generalizes better than linear regression - NDCG performance is slightly lower on train set but higher on dev set. SVM regression is better because the data may not be linearly distributed and any linear regression may not fit well. In contrast, SVM uses a non-linear kernel, which is more flexible and can fit data of a non-linear shape\footnote{\url{http://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html}}. Moreover, SVM regression introduces parameter $\varepsilon$\footnote{In $\nu$-SVR, $\varepsilon$ is replaced by $\nu$.} to make the result less sensitive to outliers\footnote{\url{http://kernelsvm.tripod.com/}}. This is a definite argument for why SVM regression generalizes better.

As for standardization, since regression is inherently to fit a set of points, there's no need to transform data to a standard distribution. This might explain why SVR with standardization is worse.

\end{document}
